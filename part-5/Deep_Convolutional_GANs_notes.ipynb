{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Convolutional GANs (DCGANs)\n",
    "\n",
    "A DCGAN is similar to a GAN, in that it has Discriminator and Generator networks.\n",
    "\n",
    "<img src=\"part-5_images/dcgan_conv.png\" alt=\"DCGAN\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN Discriminator\n",
    "\n",
    "<img src=\"part-5_images/dcgan_paper_discriminator.png\" alt=\"DCGAN Discriminator from paper\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "From the original paper: \n",
    "- a DCGAN's discriminator uses a stride=2 in order to downsample the size of the input instead of using a max pooling layer\n",
    "- all layers have a **batch normalization** (scales the output to have a mean=0  and variance=0) and **leaky relu** (relu multiplied with a coefficient) applied to their outputs \n",
    "\n",
    "Below is a snippet from the original paper on DCGAN: https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "<img src=\"part-5_images/stable_arch_dcgan.png\" alt=\"DCGAN Architecture from paper\" style=\"width: 650px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN Generator\n",
    "\n",
    "The generator is essentially upsampling the input from the input 'z' by using **transpose convolutional** layers. Leaky relu, batch normalization and a tanh are applied in the original paper.\n",
    "\n",
    "<img src=\"part-5_images/dcgan_paper_generator.png\" alt=\"DCGAN Generator\" style=\"width: 650px;\"/>\n",
    "\n",
    "The way a DCGAN is shaped is pretty similar to the autoencoder, in the sense of downsampling and upsampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "Batch normalization paper: https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "It's called \"batch\" normalization because, during training, we normalize each layer's inputs by using the mean and standard deviation (or variance) of the values in the current batch. These are sometimes called the batch statistics.\n",
    "\n",
    "Why does this help? It is already known that normalizing data helps the network converge faster. The way to think about it is that we are normalizing the output from layer and then that normalized output is passed into the next layer. Instead of layers think of them as smaller neural networks. \n",
    "\n",
    "From a mathematical perspective, batch norm helps with **internal covariate shift** refers to the change in the distribution of the inputs to different layers. It turns out that training a network is most efficient when the distribution of inputs to each layer is similar.\n",
    "\n",
    "We can make these inputs more consistent and thus reduce oscillations that may happen in gradient descent calculations. \n",
    "\n",
    "For a more in-depth exposure to batch norm, page 313 from http://www.deeplearningbook.org/contents/optimization.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Batch Normalization Layers to a PyTorch Model\n",
    "\n",
    "- Layers with batch normalization do not include a bias term. So, for linear or convolutional layers, you'll need to set bias=False if you plan to add batch normalization on the outputs.\n",
    "- You add the batch normalization layer before calling the activation function, so it always goes layer > batch norm > activation.\n",
    "- Finally, when you tested your model, you set it to .eval() mode, which ensures that the batch normalization layers use the populationrather than the batch mean and variance\n",
    "\n",
    "The reason why bias is not added is because we want the mean to be 0, therefore we do not want to add an offset (bias) that will deviate from 0. \n",
    "\n",
    "#### Benefits of Batch Normalization\n",
    "\n",
    "1. Networks train faster (it converges much more quickly even though the training time increases because there are more calculations involved)\n",
    "2. Allows higher learning rates (increases the speed at which networks train)\n",
    "3. Makes weights easier to initialize \n",
    "4. Makes more activation functions viable\n",
    "5. Simplifies the creation of deeper networks\n",
    "6. Provides a bit of regularization\n",
    "7. May give better results overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
