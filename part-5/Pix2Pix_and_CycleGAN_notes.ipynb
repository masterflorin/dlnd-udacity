{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Image Translation\n",
    "\n",
    "Is a specific task that involves moving an image from one domain to another.\n",
    "\n",
    "<img src=\"part-5_images/image2image_translation.png\" alt=\"Image to Image translation\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Loss Functions\n",
    "\n",
    "One of the most challenging aspects in deep learning is finding a good loss function, also known as the **objective function.** This is often expressed as a function that measures the difference between a prediction $\\hat{y}$ and a true target y.\n",
    "\n",
    "A very common function is the cross entropy loss, where for binary classification there is the binary cross entropy loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANs recap\n",
    "\n",
    "<img src=\"part-5_images/gans_recap.png\" alt=\"GANs recap\" style=\"width: 500px;\">\n",
    "\n",
    "*Latent means \"hidden\" or \"concealed\". In the context of neural networks, a latent space often means a feature space, and a latent vector is just a compressed, feature-level representation of an image!*\n",
    "\n",
    "For example, when you created a simple autoencoder, the outputs that connected the encoder and decoder portion of a network made up a compressed representation that could also be referred to as a latent vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pix2Pix Generator (keyword: conditional mapping)\n",
    "\n",
    "While a GAN aims to generate a realistic image from a latent factor z, in a image to image translation task, the goal is to **locate an input image and produce a desired output.**\n",
    "\n",
    "<img src=\"part-5_images/paired_img_data.png\" alt=\"Paired image dataset\" style=\"width: 500px;\">\n",
    "\n",
    "The architecture can be found in the original paper: https://arxiv.org/pdf/1611.07004.pdf\n",
    "\n",
    "To be more precise, we want the generator (G) to learn the mapping from x to y, where G is a neural network and we want G(x) to generate an output that is indistinguishable from y. That's why the dataset used is a pair of x, y. \n",
    "\n",
    "This is what is known as **CGAN (Conditional Generative Adversarial Network)** to learn the mapping from input to an output image and while it's composed of a Generator and a Discriminator there are a few differences.\n",
    "\n",
    "**How does the generator actually work?**\n",
    "\n",
    "<img src=\"part-5_images/cgan_generator.png\" alt=\"CGAN Generator\" style=\"width: 500px;\">\n",
    "\n",
    "Instead of using usual neural network that generates from input z -> a representation, the CGAN applies a transformation that is similar to an autoencoder. Essentially, the encoder learns how to distill some information about the content of the input image and encode it into a smaller feature representation then the decoder reverses the operations by looking at the sketch image representation and output something realistic.\n",
    "\n",
    "In short, it takes an input image and based on that it produces a target image as realistic as possible.\n",
    "\n",
    "Pix2pix high resolution paper: https://tcwang0509.github.io/pix2pixHD/\n",
    "\n",
    "### Pix2Pix Discriminator\n",
    "\n",
    "The discriminator is modified in such a way that instead of identifiying as fake or real images it looks at a pair of input-output images and **output a value for a fake pair or a real pair**.\n",
    "\n",
    "<img src=\"part-5_images/cgan_discriminator.png\" alt=\"CGAN Discriminator\" style=\"width: 500px;\">\n",
    "\n",
    "Based on a pair of images, it tries to determine whether the image is generated or not. The generator's weights are also adjusted based on the output of the discriminator. The discriminator is acting as the loss function and is conditional on both the input and output images to the generator which is why this is called **conditional gan.** \n",
    "\n",
    "A Pix2Pix summary: https://neurohive.io/en/popular-networks/pix2pix-image-to-image-translation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGANs & Unpaired Data (keyword: cycle mapping, consistency)\n",
    "\n",
    "Paired data isn't always available and in general it can add quite a significant cost to the project as it requires manual labor. What CycleGAN attempts to solve is by working with unpaired data. In short, it maps X -> Y and Y -> X making a cycle. e.g. if you take a sentence in english and you translate it in french, then you should be able to translate it back to english from french and have the same sentence.\n",
    "\n",
    "Considerations for unpaired image data:\n",
    "\n",
    "<img src=\"part-5_images/unpaired_image_data.png\" alt=\"Unpaired Image Data\" style=\"width: 500px;\">\n",
    "\n",
    "The risk is that for multiple input data it might map it to the same output domain, a problem known as **mode collapse**. \n",
    "\n",
    "There are many mappings from X to Y and there is only 1 constraint in place in pix2pix model for example which is why the additional constraint of ensuring that G is in the correct mapping, another mapping is added (inverse mapping) Y -> X. This is known as **Cycle Consistency constraint.**\n",
    "\n",
    "<img src=\"part-5_images/cycle_consistency.png\" alt=\"Cycle Consistency\" style=\"width: 500px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle Consistency Loss\n",
    "\n",
    "The complete loss function for a CycleGAN is as follows:\n",
    "\n",
    "<img src=\"part-5_images/cycle_cons_loss.png\" alt=\"Cycle Consistency loss\" style=\"width: 500px;\">\n",
    "\n",
    "Let's break it down.\n",
    "\n",
    "- first two elements represent the weighted combination of adversarial losses\n",
    "- lambda is the weight value that controls the rate of importance of these terms \n",
    "- the last element represens the cycle consistency losses\n",
    "\n",
    "In order to make the mappings work the term **cycle consistency loss** is added. Actually, two are added to make the mappings (X->Y, Y->X) act as tracks for one another.\n",
    "\n",
    "The cycle consistency loss is composed of the **forward consistency loss** and the **backwards consistency loss**. \n",
    "\n",
    "A loss term such as foward consistency loss is a measure of how much of these mappings contradict with one another and they are sometimes called **reconstruction errors**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN training and model shortcomings\n",
    "\n",
    "CycleGAN works on the assumption that style can be separated from content and this is what the convolutional layers are learning what to do. \n",
    "\n",
    "<img src=\"part-5_images/cyclegan_training.png\" alt=\"CycleGAN Training\" style=\"width: 550px;\">\n",
    "\n",
    "\n",
    "One of the major shortcomings is that it will only show one version of a transformed output even if there are multiple, possible outputs.\n",
    "\n",
    "Most of the time a simple CycleGAN produces low-resolution images, however there is research done concerning this aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beyond CycleGAN\n",
    "\n",
    "Paired CycleGAN provides a way of to apply multiple styles to a given image.\n",
    "\n",
    "Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data: https://arxiv.org/abs/1802.10151\n",
    "\n",
    "StarGAN, applying to more than two domains: https://github.com/yunjey/StarGAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
