{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How GANs work\n",
    "\n",
    "Essentially, there are two networks that are in competition with each other:\n",
    "\n",
    "<img src=\"part-5_images/basic_gan.png\" alt=\"GANs\" style=\"width: 650px;\"/>\n",
    "\n",
    "1. Generator - is a neural network that it takes in random noisy data and tries to output a a reshaped noise that has a realistic structure.\n",
    "\n",
    "Training process is trained in a unsupervised manner, we show the model a lot of images and ask the model to output a lot of images from that probability distribution.\n",
    "\n",
    "2. Discriminator - is a regular neural network classifier that has the role of guiding the generator towards outputing realistic output. During training it's shown 50% of the time real data and 50% fake data so it's trained to assign a probability near 1 to real images and close to 0 to fake data.\n",
    "\n",
    "The generator's is forced produce better and better output in order to fool the discriminator.\n",
    "\n",
    "The original paper for GANs: https://arxiv.org/pdf/1406.2661.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Games and Equilibria\n",
    "\n",
    "In a very simple example:\n",
    "\n",
    "- the cost for the discriminator is the negative of the cost for the generator.\n",
    "- the generator wants to minimize the cost function and the generator to maximize the cost.\n",
    "\n",
    "If both networks are large enough with mathematical tools from game theory it can be shown that there is an equilibrium where the generator density is equal to the true data density and the discriminator outputs one half everywhere.\n",
    "\n",
    "GANs are usually trained by running two optimization algorithms at the same time, each minimizing a player's cost with respect to the parameters. However, they do not necessarily find an equilibrium of the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Training GANs\n",
    "\n",
    "In general when we train a GAN:\n",
    "\n",
    "Discriminator training\n",
    "\n",
    "    Compute the discriminator loss on real, training images\n",
    "    Generate fake images\n",
    "    Compute the discriminator loss on fake, generated images\n",
    "    Add up real and fake loss\n",
    "    Perform backpropagation + an optimization step to update the discriminator's weights\n",
    "\n",
    "Generator training\n",
    "\n",
    "    Generate fake images\n",
    "    Compute the discriminator loss on fake images, using flipped labels!\n",
    "    Perform backpropagation + an optimization step to update the generator's weights\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"part-5_images/gan_architecture_example.png\" alt=\"GAN architecture\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Activation functions:** \n",
    "- Leaky ReLU makes sure the gradient can flow through the entire architecture\n",
    "- tanh - a popular choice for the output of the generator which means a rescaling (-1, 1).\n",
    "- sigmoid - used to enforce the constraint of output as a probability for discriminator\n",
    "\n",
    "<img src=\"part-5_images/stable_loss_gans.png\" alt=\"BCE stable\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "**Two optimization algorithms**\n",
    "- Adam is a good choice (also used in DCGANs)\n",
    "- a Binary Cross Entropy Loss (BCELoss) is used to calculate the loss\n",
    "- For the BCE, we need to use the BCELosswithLogits this helps the discriminator generalize better\n",
    "- For the generator loss you want to set up another BCE but with the labels flipped\n",
    "\n",
    "To scale up classifiers to work with larger images convolutional networks are used.\n",
    "\n",
    "<img src=\"part-5_images/gan_conv.png\" alt=\"GAN Convolution\" style=\"width: 600px;\"/>\n",
    "\n",
    "Use Batch Normalization in most layers except on the input and output of the generator.\n",
    "\n",
    "Improved training techniques for GANs: https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bea0c6a_improved-training-techniques/improved-training-techniques.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The universal approximation function\n",
    "\n",
    "The universal approximation theorem states that a feed-forward network with a single hidden layer is able to approximate certain continuous functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09\n",
    "    \n",
    "https://skymind.ai/wiki/generative-adversarial-network-gan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
