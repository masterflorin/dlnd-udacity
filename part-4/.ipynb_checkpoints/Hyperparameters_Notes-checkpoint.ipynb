{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters can be divided in two categories:\n",
    "\n",
    "1. **optimizer hyperparameters** - focused on optimization and training process rather than the model\n",
    "    - learning rate\n",
    "    - mini-batch size\n",
    "    - epochs\n",
    "2. **model hyperparameters** - involved in the structure of the model\n",
    "    - number of layers\n",
    "    - model specific for architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer hyperparameters\n",
    "\n",
    "### Learning Rate (most important)\n",
    "\n",
    "The learning rate acts as a multiplier we use to push the weights in the right direction.\n",
    "\n",
    "- Good starting point: 0.01\n",
    "- Common lr: 0.1, 0.01, 0.001, 0.0001, 0.00001\n",
    "\n",
    "<img src=\"part-4_images/simple_lr_example.png\" alt=\"Simple (Ideal) LR example\" style=\"width: 650px;\"/>\n",
    "\n",
    "Learning Rate Decay - a technique that decreases the learning rate by a certain factor. \n",
    "\n",
    "There are also smart algorithms such as Adaptive learning rate that can increase/decrease the learning rate depending on the training.\n",
    "\n",
    "Sources to understand gradient based methods:\n",
    "- [Adam](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "- [Intro to gradient based methods](https://tao.lri.fr/tiki-download_wiki_attachment.php?attId=954)\n",
    "- [Methods for convex optimization](https://ppasupat.github.io/a9online/uploads/proximal_notes.pdf)\n",
    "- [Adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Size\n",
    "\n",
    "Mini-batch size - number of training examples\n",
    "\n",
    "Most often the mini-batch size is between: 1, 2, 4 ... 256, in powers of two.\n",
    "- too small -> too slow\n",
    "- too large -> too computationally expensive\n",
    "\n",
    "In practice smaller mini-batch size are more noisy but they are more useful as it can prevent the gradient to get stuck into a local minima. If we change the batch size, then we have to adjust the learning rate.\n",
    "\n",
    "In-depth analysis of different hyperparameter choice evaluation, including [mini-batch size](https://arxiv.org/pdf/1606.02228.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Training Iterations / Epochs\n",
    "\n",
    "The metric we need to focus on is the validation error.\n",
    "\n",
    "The number of training iterations is a hyperparameter we can optimize automatically using a technique called early stopping.\n",
    "\n",
    "For PyTorch, early stopping isn't directly implemented but can be found through an [additional library](https://pytorch.org/ignite/handlers.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameters\n",
    "\n",
    "### Number of Hidden Units / Layers\n",
    "\n",
    "- the hidden units is what helps the model learn it needs to have enough capacity to learn how to approximate the function\n",
    "\n",
    "\"in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers).\" ~ Andrej Karpathy in https://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "Additional resource: http://www.deeplearningbook.org/contents/ml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Hyperparameters\n",
    "\n",
    "In practice, it has been shown that LSTM and GRUs perform better than regular RNNs. However, between the two that remains to be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter resources\n",
    "\n",
    "- [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533)\n",
    "- [Deep Learning book - chapter 11.4: Selecting Hyperparameters](http://www.deeplearningbook.org/contents/guidelines.html)\n",
    "- [Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters?](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters)\n",
    "- [Efficient BackProp (pdf)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "#### Specialized resources\n",
    "\n",
    "- [How to Generate a Good Word Embedding?](https://arxiv.org/abs/1507.05523)\n",
    "- [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
