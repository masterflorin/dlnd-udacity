{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Weight Matrix/Lookup Table\n",
    "\n",
    "What embedding as a concept does is help us save a lot of computational power. For instance, let's take a really big text that we are encoding it using one-hot encoding in order to pass it into our network. The way it works is that the vector of each character is multiplied with the weights resulting in a huge matrix where most of characters are zero making it computationally inefficient.\n",
    "\n",
    "<img src=\"part-4_images/one-hot-encoding-vector.png\" alt=\"Vector-matrix multiplication\" style=\"width: 550px;\"/>\n",
    "\n",
    "This is where embedding can help. Embeddings are just a fully connected layer so can use them as lookup table to speed up by taking the position of the word in the vector and select the value from the embedding weights matrix based on that position. since If we'd multiply that vector with the entire matrix we'd still end up with the same value but a lookup would be much faster.\n",
    "\n",
    "This process is called an **embedding lookup** and the number of hidden units is the **embedding dimension**.\n",
    "\n",
    "<img src=\"part-4_images/embedding-lookup.png\" alt=\"Embedding lookup\" style=\"width: 550px;\"/>\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "There are two important papers on the subject:\n",
    "-  [Word2Vec paper](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf)\n",
    "- [Distributed representations](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf)\n",
    "\n",
    "The Word2Vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
