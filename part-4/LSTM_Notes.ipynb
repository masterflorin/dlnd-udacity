{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN vs LSTMs\n",
    "\n",
    "- Compared to regular NN they can make use of information as input from another hidden layer neuron in order to improve its prediction\n",
    "- RNNs suffer from the vanishing gradient problem\n",
    "- They're great with handling small time steps 8-10 but their effectiveness drops significantly after that\n",
    "\n",
    "\n",
    "### Basics of LSTM (keyword: cell, gates)\n",
    "\n",
    "A rough version of what an LSTM does is summarized in the image below:\n",
    "\n",
    "<img src=\"part-4_images/lstm_basic.png\" alt=\"LSTM basic\" style=\"width: 650px;\"/>\n",
    "\n",
    "It's composed of 4 gates which are essentially performing the calculations.\n",
    "\n",
    "However, a more accurate depiction of how LSTM cell looks like involves several activation functions and an addition, with the cell being responsible for passing LTM and STM information, to the next cell if available.\n",
    "\n",
    "We have to inputs, and two outputs.\n",
    "\n",
    "<img src=\"part-4_images/lstm_cell.png\" alt=\"LSTM\" style=\"width: 550px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Gate (multiplication, tanh, sigmoid, ignore factor)\n",
    "\n",
    "What this gate does is it takes Short Term Memory ($STM_{t-1}$) and the event (E), it combines them and removes some of the information out of it using an ignore factor ($i_{t}$).\n",
    "\n",
    "- STM and E are combined\n",
    "- then multiplied with the Weight matrix plus the bias then it gets squished by a tanh function\n",
    "- the result of that $N_{t}$ gets multiplied with an ignore factor ($i_{t}$)\n",
    "- $i_{t}$ is essentially another neural network with input STM and E but with a new matrix, and an activation function is a sigmoid to keep it between 0 and 1\n",
    "\n",
    "<img src=\"part-4_images/learn-gate.png\" alt=\"Learn Gate\" style=\"width: 450px\"/>\n",
    "\n",
    "### Forget Gate (multiplication, sigmoid, forget factor)\n",
    "\n",
    "The forget gate contributes to preserving only the essential information from the LTM input, a feature that is accomplished by multiplying LTM as input with the forget factor. The forget factor is obtained with the help of the STM.\n",
    "\n",
    "<img src=\"part-4_images/forget-gate.png\" alt=\"Forget Gate\" style=\"width: 450px\"/>\n",
    "\n",
    "### Remember Gate (addition)\n",
    "\n",
    "In this step, the outputs from the learn gate and the forget gate are added which results into a  new LTM.\n",
    "\n",
    "<img src=\"part-4_images/remember-gate.png\" alt=\"Remember Gate\" style=\"width: 450px\"/>\n",
    "\n",
    "### Use Gate (tanh, sigmoid, multiplication, output)\n",
    "\n",
    "It receives as input from Learn Gate and Forget Gate to generate a new output by multiplication.\n",
    "\n",
    "<img src=\"part-4_images/use-gate.png\" alt=\"Use Gate\" style=\"width: 450px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other architectures\n",
    "\n",
    "#### Gated Recurrent Unit (GRU)\n",
    "\n",
    "<img src=\"part-4_images/gru.png\" alt=\"GRU\" style=\"width: 450px\"/>\n",
    "\n",
    "#### Peephole connections\n",
    "\n",
    "It addresses the issue of LTM being used as input for the calculation of the forget factor. The original forget factor is concatenated with LTM matrix.\n",
    "\n",
    "<img src=\"part-4_images/peephole-connections.png\" alt=\"Peephole connections\" style=\"width: 450px\"/>\n",
    "\n",
    "This technique can be applied to any of the forget-type nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview LSTM\n",
    "\n",
    "- Is not affected by vanishing gradient problem\n",
    "- An LSTM cell is composed of 4 gates which determine the LTM and the STM (output)\n",
    "- All of these 4 gates use activation function to squish the input from 0 to 1 or from -1 to 1 (tanh) making them suitable for backpropagation.\n",
    "- As information is passed between these gates, they decide what to remember and what to discard, which information is passed on to the next gate\n",
    "- Addition and multiplication play different roles which essentially contribute to solving the vanishing gradient\n",
    "\n",
    "Article that provides an overview of [LSTM](https://skymind.ai/wiki/lstm#long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
