{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "There are different aspects to model training and weight initialization is an important part of it, this is what gives a model the best chance to achieve great results during training.\n",
    "\n",
    "The best way to get a proper initialization would be to compare two models at a time. \n",
    "\n",
    "Udacity provided a notebook with exercises that's really great for experimenting and learning different approach.\n",
    "\n",
    "### Constant weights\n",
    "\n",
    "One approach to the weight initialization would be to set all the weights to 1 or 0. In theory, this would be rather simple to work with as there are only two values and you can have some idea of what to expect. \n",
    "- setting all weights to 0 would mean that backpropagation cannot differentiate between the errors as everything would be constant right from the start, nothing would change.\n",
    "- setting all weights to 1 would mean that the starting values are really high resulting a large training loss, and since 1 is a pretty good number for multiplying for constant values then backpropagation would not work well on this sort of value.\n",
    "\n",
    "As a reflection, we could consider working with values which are not fix and by this I mean **randomness**. Better weights might be selected randomly from within a specified range, and we would have less problems with having too high or too low values since they would be distributed across the network. This would help backpropgation work significantly better. However, what sort of random weights are talking about? On what distribution and how would we define that?\n",
    "\n",
    "### Random Uniform\n",
    "\n",
    "For a baseline model, we can generate random weights on a uniform distribution between (0.0 and 1.0), this would ensure that there are very low chances of numbers repeating thus we would end up with mostly unique values. This will give much better results than applying constant weights. \n",
    "\n",
    "### General Rule\n",
    "\n",
    "A range between 0 and 1 can be quite large and it's not the best range to choose from. We know that in the hidden layers the weights act as multipliers it would make more sense that the number of weights would be related to how many input features it should see. As a result, our starting training loss would be much smaller and manageable if our range would be narrower.\n",
    "\n",
    "The rule of thumb for setting the weights in a neural network is to set them to be close to zero without being too small.\n",
    "\n",
    "It's good practice is to start your weights in the range of $[-y, y]$, where $y=1/\\sqrt(n)$ (ùëõ is the number of inputs to a given neuron). \n",
    "\n",
    "We do not have to do a lot of experimentation to see the differences between these two ranges.\n",
    "\n",
    "### Normal Distribution\n",
    "\n",
    "With normal distribution initialization, our weights are much closer to the mean, this would make the numbers closer to the mean as being more likely to be selected.\n",
    "\n",
    "The normal distribution gives us pretty similar behavior compared to the uniform distribution, in a simple example, however this can be attributed to smaller networks where so in a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles.\n",
    "\n",
    "### More weight initialization in PyTorch\n",
    "\n",
    "PyTorch has default weight initialization behavior for every kind of layer. You can see that linear layers are initialized with a uniform distribution (uniform weights and biases) in the [module source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html).\n",
    "\n",
    "PyTorch has various ways for initializing the weights in the [documentation](https://pytorch.org/docs/stable/nn.html#torch-nn-init).\n",
    "\n",
    "### Additional materials, papers\n",
    "\n",
    "[Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "[Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852v1.pdf)\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v2.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
