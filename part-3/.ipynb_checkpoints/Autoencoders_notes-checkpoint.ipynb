{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an Autoencoder?\n",
    "\n",
    "An Autoencoder is a model that can make use of a CNN's ability to compress the data into a flat vector / feature vector. We can think of it as a smart encoder that learns compression and decompression algorithms from the data. As an example, if you had a file format that was highly dimensional or noisy then you could use an autoencoder to get a file that you are able to work with.\n",
    "\n",
    "The main ability of an autoencoder is that it's able to compress the data while maintaining its content which makes it possible to use the compress representation of the input.\n",
    "\n",
    "![Autoencoder](part3_images/autoencoder.png)\n",
    "\n",
    "### Linear autoencoder\n",
    "\n",
    "This is a simple autoencoder that uses an MLP with with a few linear layers for encoder and decoder. The number of layers depends on the problem you trying to solve.\n",
    "\n",
    "Another thing to note here is you might want to consider a different loss function such as MSELoss because it's suitable for comparing pixel quantities rather than probabilities. You are also interested in only the images and not the labels, same goes for validation set, you're mostly focusing on training and then you're using test to visualize the reconstruction.\n",
    "\n",
    "**Key point**: we are comparing the images that resulted from the reconstruction with the original ones so we're not interested in accuracy like in usual applications.\n",
    "\n",
    "### Upsampling\n",
    "\n",
    "Encoder performs what is called **downsampling** as it is compressing the data into a flat vector. Conversely, the decoder is doing an **upsampling** through a transpose convolutional layer or sometimes called deconvolutional layer. Note: deconv layer doesn't strictly mean that we are undoing a convolution. It is essentially reversing the downsampling by increasing the spatial dimensions of a compressed input so that you get to the original dimensions of the input \n",
    "\n",
    "### Transpose convolutional layer\n",
    "\n",
    "Beginning from the compressed representation, a filter with a given size and stride passes over the input and multiplies the pixel with its weight resulting a representation. Below is the result after the convolution went over all points. The interesting part is that this pixel representation will overlap with another convolution and what happens is that the overlapping section/edges get summed together.\n",
    "\n",
    "![Transpose conv layer](part3_images/transpose_conv.png)\n",
    "\n",
    "There are options to add/substract the padding from the output as above but the most common solution is a 2x2 filter and stride of 2 to double the size of the kernel. \n",
    "\n",
    "![Transpose conv layer_square](part3_images/transpose_conv_2.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
