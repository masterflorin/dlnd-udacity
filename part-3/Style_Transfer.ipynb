{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Style Transfer?\n",
    "\n",
    "A technique that allows you to apply the style of one image to another image of your choice with the help of CNN.\n",
    "\n",
    "#### Style and content separation\n",
    "\n",
    "To perform style transfer we are interested in combining the content of an image with the style of an image, merge these two into another image. \n",
    "\n",
    "**Where does the content come from? (Keywords: content, object)**\n",
    "\n",
    "Essentially, what this means is that we'll use the content (features) of a model of a input image from the later layers. It's know that the later layers are focused on the content rather than the details concerning texture or colors. Some models, e.g. VGG-19 has been quite successful at this task.\n",
    "\n",
    "![Content representation](part3_images/content-representation.png)\n",
    "\n",
    "**What about the style? (Keywords: texture, color)** \n",
    "\n",
    "We can think about style as being captured by the feature maps that look at spatial correlations within the layer. So they are special. For each feature map (depth level of conv layer) we can measure how strongly it's detected feature relate to the other feature maps in that layer. Those similarities and differences are going to tell us information about **texture and color**, but not about placement or identity of the objects.\n",
    "\n",
    "Both the style and content image pass through the network with feedforward until they reach a certain conv stack that acts as a feature extractor for both content and style. \n",
    "\n",
    "**How do we create the target image (combination)?**\n",
    "\n",
    "As the target image is essentially created by combining the outputs from extractors from content and style, the target image's representation is compared to that of the content image. To formalize the comparison a loss function that compares how far the image are from each other. In the example below the MSE is used.\n",
    "\n",
    "$$ \\zeta_{content} = \\frac{1}{2}\\sum(T_{c}-C_{c})^2$$\n",
    "\n",
    "The goal will be to **minimize loss** similar to the way loss is used in optimization. There is one big distinction, as the goal is to update the appearance of the target until it matches that of the content image we are not training the CNN at all. The model is used as a feature extractor with backpropagation as the method of minimizing the loss based on the function defined earlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
