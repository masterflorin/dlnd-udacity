{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "\n",
    "So far the way we used to see how well our model generalizes on a dataset was to use training and validation dataset, however this can result in a heavy bias towards the result in the validation set thus when deciding which model to choose it would be preferred to use another dataset, the test set. \n",
    "\n",
    "![Model validation](part3_images/model_validation.png)\n",
    "\n",
    "We create a validation set to:\n",
    "\n",
    "- Measure how well a model generalizes, during training\n",
    "- Tell us when to stop training a model; when the validation loss stops decreasing (and especially when the validation loss starts increasing and the training loss is still decreasing)\n",
    "\n",
    "![Model epochs](part3_images/model_training_validation.png)\n",
    "\n",
    "You can check an [example](convolutional-neural-networks/mnist-mlp/mnist_mlp_solution_with_validation.ipynb) for model that uses both validation and test datasets.\n",
    "\n",
    "### Image Classification Steps\n",
    "\n",
    "Below is an overview of the image classification process using deep learning.\n",
    "\n",
    "![Image classification steps](part3_images/image_class_steps.png)\n",
    "\n",
    "\n",
    "### MLP vs CNN\n",
    "\n",
    "While on MNIST dataset, an ML can do just as good of a job as a CNN the differences are very large in practice. There are are a few reasons why it has a similar performance on MNIST:\n",
    "- dataset is clean\n",
    "- well-centered\n",
    "- pre-processed\n",
    "- roughly the same size\n",
    "\n",
    "The issue with ML is that it requires a flat vector therefore losing te knowledge of spatial arrangements of the pixels. That's exactly where a CNN can shine, it is designed to work with multi-dimensional data and recognize patterns in the image thanks to its architecture. Link to ![MNIST database](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "\n",
    "### Local Connectivity\n",
    "\n",
    "![MLP vs CNN](part3_images/mlp_vs_cnn.png)\n",
    "\n",
    "Based on pros and cons, it can be argued that we need a completely different approach to handling the image input if we were to take advantage of the spatial patterns. \n",
    "\n",
    "The case in point being, is every node needed to be connected with the input? No, and we'll argue why. We can focus our nodes into specific regions thus it would make our layers locally connected with only the parameters they need rather than a densely connected layer so each hidden node sees about a quarter of the original image as in the example. \n",
    "\n",
    "Every hidden node still reports to the output layer where the output layer combines the discovered patterns learned separately in each region.\n",
    "\n",
    "This makes it less prone to overfitting and we can also use a 2D matrix as input.\n",
    "\n",
    "More layers -> more complex patterns.\n",
    "\n",
    "What is more useful, to have each of the hidden nodes within a collection to share a common group of weights thus enabling the ability of regions to share the same kind of information.\n",
    "\n",
    "![Local connectivity](part3_images/local_connectivity.png)\n",
    "\n",
    "### Filters and the convolutional layer\n",
    "\n",
    "![Filters and the convolutional layers](images/filters_conv_layers.png)\n",
    "\n",
    "The Convolutional layer is a special type of NN that remembers **spatial patterns**. Several filters with different purposes are applied to input to extract edges, colors etc.\n",
    "\n",
    "#### Filters and edges\n",
    "\n",
    "Filters applied to the image can enable us to detect **spatial patterns** which you can think of as the **color or shape**, the same way you can detect **color intensity** which enables us to detect the shape boundary.\n",
    "\n",
    "#### Frequency in images\n",
    "\n",
    "In images, frequency is the rate of change\n",
    "- level of brightness changes quickly from one pixel to the next\n",
    "- a low frequency may be one that is relatively uniform in brightness or changes very slowly\n",
    "- high frequency in an image means that intensity changes a lot\n",
    "\n",
    "To better picture this, think about the sound where frequency refers to how fast a sound wave is oscillating.\n",
    "\n",
    "![Frequency](images/frequency_images.png)\n",
    "\n",
    "Essentially, **high frequency components also correspond to the edges of objects** in images which can help us classify those objects.\n",
    "\n",
    "#### High pass filters\n",
    "\n",
    "**Filters** are used to:\n",
    "1. filter out unwanted information\n",
    "2. amplify features (e.g. objected boundaries)\n",
    "\n",
    "**High-pass filters** are used to:\n",
    "- sharpen an image\n",
    "- enhance high-frequency parts of an image (e.g. detect a line)\n",
    "\n",
    "**Edge detection**, edges represent areas within an image where the intensity changes very quickly and often indicate object boundaries. How does it work? This is done with the help of convolutional kernels. \n",
    "\n",
    "![Edge detection filter](part3_images/edge_det_filter.png)\n",
    "\n",
    "**Kernel** is just a matrix that modifies an image. Essentially for edge detection is important that all elements sum 0 because the filter is calculating the difference or change between neighbouring pixels. \n",
    "\n",
    "If it is not 0 the value can be positive or negative which in turn brightens or darkens the entire filtered image. \n",
    "\n",
    "Convolution kernel representation: $K * F(x,y) = output image$ where * does not represent multiplication.\n",
    "\n",
    "![Convolutional kernels](part3_images/convolutional_kernels.png)\n",
    "\n",
    "How do convolutional kernels work? The kernel is placed over the pixel in the center, then a multiplication is done between the weights of the filter and the values of the filters, then we sum all of them and we get the pixel value in the output image. \n",
    "\n",
    "This sum becomes the value for the corresponding pixel at the same location in the output image. This gets repeated for every pixel position in the image.\n",
    "\n",
    "**For edge detection**:\n",
    "- center pixel is most important\n",
    "- followed by top, bottom left and right (these increase the contrast of the image)\n",
    "- corners receive no weights because they are the farthest from the center\n",
    "\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge?\n",
    "- Extend The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "- Padding The image is padded with a border of 0's, black pixels.\n",
    "- Crop Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.\n",
    "\n",
    "![Examples of conv kernels](part3_images/examples_of_conv_kernels.png)\n",
    "\n",
    "In this example above, option **d** would make a proper choice for finding and enhancing the horizontal edges, while **c** could be used for vertical.\n",
    "\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "Convolutional layer is what preserves spatial information and learn to extract features such as edges of objects.\n",
    "\n",
    "The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image. \n",
    "\n",
    "![Layers in a CNN](part3_images/layers_cnn.png)\n",
    "\n",
    "**4 kernels = 4 filtered images = depth of 4!** => forms a convolutional layer.\n",
    "\n",
    "Filters are used to capture characteristics of the image and it's common to have tens to hundreds of collections of nodes each with their own filters. \n",
    "\n",
    "![Input to conv](part3_images/input_to_conv_layer.png)\n",
    "\n",
    "What we can see above are the filters specified for vertical and horizontal edges wich are then convolving over the input layer thus resulting in a set of nodes (4 in this case, one per filter) called as **feature maps** or **activation maps**.\n",
    "\n",
    "To visualize the filtered outputs of the convolutional layers check this [notebook](convolutional-neural-networks/conv-visualization/conv_visualization.ipynb).\n",
    "\n",
    "Below is an example of the computation of a single node in a convolutional layer for an input layer (RGB image).  \n",
    "\n",
    "![Conv layer in RGB](part3_images/conv_layer_rgb.png)\n",
    "\n",
    "Note, a trained CNN will learn the values of weights for filters.\n",
    "\n",
    "The 3D array obtained can be used as input to another convolutional layer to discover patterns within patterns and so on.\n",
    "\n",
    "Essentially, they aren't very different from dense layers. Moreover, inference works the same, both having weights and biases and initialized weights randomly, loss functions. What makes a CNN special is the ability to determine what kind of patterns it needs to detect.\n",
    "\n",
    "### Stride and padding\n",
    "\n",
    "The behavior of the convolutional layer can be done by specifying the number of filters and the size of each filter. e.g. to increase number of nodes, increase number of filters. To increase the size of detection patterns, increase the size of the filter.\n",
    "\n",
    "**Stride** refers to the amount by which a filter slides over the image.\n",
    "\n",
    "**Padding** is the process of adding 0s at the edges in order for the filter to move and capture more regions. While the option of discarding the edges is tempting, it is likely that we lose information from those regions.\n",
    "\n",
    "### Pooling\n",
    "\n",
    "What pooling does is help us reduce the dimensionality of the convolutional layers. Essentially, a complicated dataset requires many filters -> larger stack -> large dimensionality -> more parameters -> overfitting.\n",
    "\n",
    "Commonly, there are two types of pooling:\n",
    "\n",
    "#### Max pooling \n",
    "\n",
    "Takes the stack of feature maps as input, then use a defined window size and stride, then takes the maximum value of each window, then the output is stacked with the same number of feature maps but each feature map has been reduce in width and height. It works in a similar way as compression where only the most important pixels are selected. Below you can see a visualizatio, in the example below the convolutional layer has been reduced to half.\n",
    "\n",
    "![Max pooling](part3_images/max_pooling.png)\n",
    "\n",
    "#### Average pooling\n",
    "\n",
    "The other option,  it's as the name says, performs an averaging of the values inside a filter. E.g. In a 2x2 window, this operation will see 4 pixel values, and return a single, average of those four values, as output! This type is not very commonly used for image classification, being preferred for smoothing applications instead.\n",
    "\n",
    "### Capsule Networks\n",
    "\n",
    "Are a special type of network in that it detects parts of the objects in an image and represent spatial relationships between the parts. While CNNs also focus on the spatial patterns it discards spatial information through the pooling layers thus a lot of research has been done concerning this aspect. \n",
    "\n",
    "Capsule networks are able to recognize the same object, like a face, in a variety of different poses and with the typical number of features (eyes, nose , mouth) even if they have not seen that pose in training data. Capsule networks are made of parent and child nodes that build up a complete picture of an object.\n",
    "\n",
    "![Capsule networks](part3_images/capsule_networks1.png)\n",
    "\n",
    "- Each node in a tree represents a single capsule.\n",
    "- Each leaf represents a single, focused observation.\n",
    "\n",
    "But what are capsules though? Are they different from a NN? Capsules are a collection of nodes, each of which contains information about a specific part; part properties like width, orientation, color, and so on. The important thing to note is that each capsule outputs a vector with some magnitude and orientation. So, they somewhat follow the principle of locally connected in the terms of different regions working on specific tasks.\n",
    "\n",
    "- Magnitude (m) = the probability that a part exists; a value between 0 and 1.\n",
    "- Orientation (theta) = the state of the part properties.\n",
    "\n",
    "These output vectors allow us to do some powerful routing math to build up a parse tree that recognizes whole objects as comprised of several, smaller parts! The magnitude is a special part property that should stay very high even when an object is in a different orientation.\n",
    "\n",
    "For a deeper dive into capsule networks, Udacity has provided a [blog post](https://cezannec.github.io/Capsule_Networks/) and a [github repo](https://github.com/cezannec/capsule_net_pytorch).\n",
    "\n",
    "### Increasing Depth\n",
    "\n",
    "Recall that a common problem in image classification is resizing, all the images are required to have the same size before anything else. Other preprocessing steps: normalization, conversion to tensor.\n",
    "\n",
    "It;s common to resize to a square with spatial dimension of power of or divisible by power of two. \n",
    "\n",
    "Input array will always be taller and wider than it is deeper. The depth represent the channels e.g RGB = 3, grayscale = 1\n",
    "\n",
    "![CNN layers](part3_images/cnn_basics.png)\n",
    "\n",
    "### The goal of a CNN architecture:\n",
    "\n",
    "- take an array and make it much deeper than it is taller and wider\n",
    "- convolutional layers are used to make the array deeper\n",
    "- max pooling to decrease the x, y dimensionality\n",
    "\n",
    "![CNN architecture](part3_images/cnn_arch_example.png)\n",
    "\n",
    "Why do we need padding? because they repersent the size of borders around the image. Essentially, when we create a convolutional alyer, we move a square filter over an image using center-pixel as the anchor. Thus, the **kernel cannot perfectly overlay the edges of the image**. \n",
    "\n",
    "The most common methods of padding are padding an image with all 0-pixels (zero padding) or padding them with the nearest pixel value.\n",
    "\n",
    "Case: a maxpooling of (2, 2) or (4, 4) where first number is the kernel size, second the stride. This will **down-sample** input layer by 4. Since we are jumping 4 pixels at a time => smaller output volumes spatially.\n",
    "\n",
    "#### Convolutional Layers in PyTorch\n",
    "\n",
    "There are 3 Hyperparameters that control the size of the output of a convolutional layer\n",
    "- `depth` - (number of filters we would like to use)\n",
    "- `stride` - (depending hon how much the filters jump this will produce smaller output volumes spatially)\n",
    "- `zero-padding` - (control spatial size of the output volumes)\n",
    "\n",
    "We typically define a convolutional layer in PyTorch using `nn.Conv2d`, with the following parameters, specified:\n",
    "\n",
    "`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)`\n",
    "\n",
    "- in_channels refers to the depth of an input. For a grayscale image, this depth = 1\n",
    "- out_channels refers to the desired depth of the output, or the number of filtered images you want to get as output\n",
    "- kernel_size is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n",
    "- stride and padding have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y\n",
    "\n",
    "Convolutional layer with filter and stride [visualization](http://iamaaditya.github.io/2016/03/one-by-one-convolution/).\n",
    "\n",
    "Example in PyTorch of CNN layers in the init function:\n",
    "\n",
    "```\n",
    "def __init__(self):\n",
    "        super(ModelName, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "              nn.Conv2d(1, 16, 2, stride=2),\n",
    "              nn.MaxPool2d(2, 2),\n",
    "              nn.ReLU(True),\n",
    "\n",
    "              nn.Conv2d(16, 32, 3, padding=1),\n",
    "              nn.MaxPool2d(2, 2),\n",
    "              nn.ReLU(True) \n",
    "         )\n",
    "```\n",
    "\n",
    "**Formula: Number of Parameters in a Convolutional Layer**\n",
    "\n",
    "The number of parameters in the convolutional layer is given by `K*F*F*D_in + K`, where\n",
    "\n",
    "- `K` - the number of filters in the convolutional layer\n",
    "- `F` - the height and width of the convolutional filters\n",
    "- `D_in` - the depth of the previous layer\n",
    "\n",
    "**Formula: Shape of a Convolutional Layer**\n",
    "\n",
    "The spatial dimensions of a convolutional layer can be calculated as: `[(W_in−F+2P)/S]+1` where\n",
    "\n",
    "- `K` - the number of filters in the convolutional layer\n",
    "- `F` - the height and width of the convolutional filters\n",
    "- `S` - the stride of the convolution\n",
    "- `P` - the padding\n",
    "- `W_in` - the width/height (square) of the previous layer\n",
    "\n",
    "Example: \n",
    "\n",
    "W_in = 7x7, F = 3x3, S = 1, P = 0\n",
    "\n",
    "This results in a 5x5 output according to the formula.\n",
    "\n",
    "Use of **zero-padding**, calculated as `P = (F - 1)/2` when S=1 ensures that the input and output volume will have the same size spatially.\n",
    "\n",
    "**Spatial constraints** exist on strides because of the spatial arrangements of the hyperparameters.\n",
    "\n",
    "Example: W=10, P=0, F=3 then it would be impossible to use a stide of 2 as it would result in 4.5 from the formula -> while it's not an integer neurons don't fit symetrically across input.\n",
    "\n",
    "#### Flattening\n",
    "\n",
    "Part of completing a CNN architecture, is to flatten the eventual output of a series of convolutional and pooling layers, so that all parameters can be seen (as a vector) by a linear classification layer. At this step, it is imperative that you know exactly how many parameters are output by a layer.\n",
    "\n",
    "1. For the following quiz questions, consider an input image that is 130x130 (x, y) and 3 in depth (RGB). Say, this image goes through the following layers in order:\n",
    "\n",
    "```\n",
    "nn.Conv2d(3, 10, 3)\n",
    "nn.MaxPool2d(4, 4)\n",
    "nn.Conv2d(10, 20, 5, padding=2)\n",
    "nn.MaxPool2d(2, 2)\n",
    "```\n",
    "\n",
    "After going through all four of these layers in sequence, what is the depth of the final output? The answer is **20** because the last convolutional layer has an output channel of 20 specified, moreover max pooling does not later depth size.\n",
    "\n",
    "2. What is the x-y size of the output of the final maxpooling layer? Careful to look at how the 130x130 image passes through (and shrinks) as it moved through each convolutional and pooling layer.\n",
    "\n",
    "Let's apply the formulas:\n",
    "- Conv1 [(130-3+2*0)/1]+1 = 32\n",
    "- MaxPool1 128/4 = 32\n",
    "- Conv2 [(32-5+2*2)/1]+1 = 32\n",
    "- MaxPool2 = 32/2\n",
    "\n",
    "Actual formula for **pooling arithmetic**: $o = \\lbrack \\frac {i - k}{s} \\rbrack + 1$ where i = input from conv, k = kernel, s = stride\n",
    "\n",
    "3. How many parameters, total, will be left after an image passes through all four of the above layers in sequence?\n",
    "\n",
    "16 * 16 * 20, as we have the size of (x,y) times depth channels.\n",
    "\n",
    "### Feature vector\n",
    "\n",
    "It is a representation that encodes only the contentof a converted input image array. Detail is descarded in later output making the outputs look very similar => feature-level representation\n",
    "\n",
    "![Feature vector](part3_images/feature_representation.png)\n",
    "\n",
    "Pay attention when passing data from Conv to Fully Connected layers as you need to calculate the size of the image after it passed through all conv/pooling layers multiplied by the number of output channels from conv.\n",
    "\n",
    "```\n",
    "# convolutional layer (sees 32x32x3 image tensor)\n",
    "self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "# convolutional layer (sees 16x16x16 tensor)\n",
    "self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "# convolutional layer (sees 8x8x32 tensor)\n",
    "self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "# max pooling layer\n",
    "self.pool = nn.MaxPool2d(2, 2)\n",
    "# linear layer (64 * 4 * 4 -> 500)\n",
    "self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "# linear layer (500 -> 10)\n",
    "self.fc2 = nn.Linear(500, 10)\n",
    "# dropout layer (p=0.25)\n",
    "self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "\n",
    "def forward(self, x):\n",
    "    # add sequence of convolutional and max pooling layers\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = self.pool(F.relu(self.conv3(x)))\n",
    "    # flatten image input\n",
    "    x = x.view(-1, 64 * 4 * 4)\n",
    "    # add dropout layer\n",
    "    x = self.dropout(x)\n",
    "    # add 1st hidden layer, with relu activation function\n",
    "    x = F.relu(self.fc1(x))\n",
    "    # add dropout layer\n",
    "    x = self.dropout(x)\n",
    "    # add 2nd hidden layer, with relu activation function\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "```\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "A process that is used to avoid overfitting because it helps the model see many new images thus becoming better at generalizing on unseen data. What we want is to make the algorithm learn an **invariant representation** of the image. \n",
    "\n",
    "Specifically, the value of the prediction shouldn't change because of:\n",
    "- angle (rotation invariance)\n",
    "- size (scale invariance)\n",
    "- shift (translation invariance)\n",
    "\n",
    "In order to achieve this:\n",
    "- we increase the training set\n",
    "- apply augmentation techniques such as rotation, translation and many others on the training set\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
