{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an Autoencoder?\n",
    "\n",
    "An Autoencoder is a model that can make use of a CNN's ability to compress the data into a flat vector / feature vector. We can think of it as a smart encoder that learns compression and decompression algorithms from the data. As an example, if you had a file format that was highly dimensional or noisy then you could use an autoencoder to get a file that you are able to work with.\n",
    "\n",
    "The main ability of an autoencoder is that it's able to compress the data while maintaining its content which makes it possible to use the compress representation of the input.\n",
    "\n",
    "![Autoencoder](part3_images/autoencoder.png)\n",
    "\n",
    "### Linear autoencoder\n",
    "\n",
    "This is a simple autoencoder that uses an MLP with with a few linear layers for encoder and decoder. The number of layers depends on the problem you trying to solve.\n",
    "\n",
    "Another thing to note here is you might want to consider a different loss function such as **MSE** because it's suitable for comparing pixel quantities rather than probabilities, it's a function that uses regression instead of probabilities. You are also interested in only the images and not the labels, same goes for validation set, you're mostly focusing on training and then you're using test to visualize the reconstruction.\n",
    "\n",
    "**Key point**: we are comparing the images that resulted from the reconstruction with the original ones so we're not interested in accuracy like in usual applications.\n",
    "\n",
    "**Code**: [Notebook](autoencoder/linear-autoencoder/Simple_Autoencoder_Exercise.ipynb) with fully connected layers.\n",
    "\n",
    "### Upsampling\n",
    "\n",
    "Encoder performs what is called **downsampling** as it is compressing the data into a flat vector. Conversely, the decoder is doing an **upsampling** through a transpose convolutional layer or sometimes called deconvolutional layer. Note: deconv layer doesn't strictly mean that we are undoing a convolution. It is essentially reversing the downsampling by increasing the spatial dimensions of a compressed input so that you get to the original dimensions of the input \n",
    "\n",
    "### Transpose convolutional layer\n",
    "\n",
    "Beginning from the compressed representation, a filter with a given size and stride passes over the input and multiplies the pixel with its weight resulting a representation. \n",
    "\n",
    "Below is the result after the convolution went over all points. The interesting part is that this pixel representation will overlap with another convolution and what happens is that the overlapping section/edges get summed together. In this case a stride of 2 was used for the output. \n",
    "\n",
    "![Transpose conv layer](part3_images/transpose_conv.png)\n",
    "\n",
    "There are options to add/substract the padding from the output as above but the most common solution is a 2x2 filter and stride of 2 to double the dimensions of the input.\n",
    "\n",
    "![Transpose conv layer_square](part3_images/transpose_conv_2.png)\n",
    "\n",
    "When not sure about how the calculations are done there is a very good explanation of how convolutional arithmethic works in this [repo](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
    "\n",
    "**Code**: [Notebook](autoencoder/convolutional-autoencoder/Convolutional_Autoencoder_Exercise.ipynb) with convolutional and transpose convolutional layers.\n",
    "\n",
    "Convolutional neural networks gives as as an output the image that is much closely resembling the original imaage compared to the autoencoder that used linear layers. However, there are still some artifacts present in some of the images. This can be attributed to how Transpose conv layer works.\n",
    "\n",
    "The solution for this is to use a technique called upsampling with nearest-neightbor interpolation coupled with convolutional layers.\n",
    "\n",
    "**Code**: [Notebook](autoencoder/convolutional-autoencoder/Upsampling_Solution.ipynb) with upsampling. \n",
    "\n",
    "### De-noising\n",
    "\n",
    "One of the most interesting things you can use an autoencoder for is de-noising.\n",
    "\n",
    "**Code**: [Exercise](autoencoder/denoising-autoencoder/Denoising_Autoencoder_Exercise.ipynb) of using it for de-noising."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
