{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "\n",
    "\n",
    "**Regression** - think about fitting a line through data. If we have the following equation that tells us how the line is drawn $y = w_{1}x + w_{2}$ the way we can adjust the line is to modify the weights. That will increase or decrease the slope of the line.\n",
    "\n",
    "In this notebook, several methods are mentioned on how to move the line to fit the data.\n",
    "\n",
    "### Absolute trick\n",
    "\n",
    "This is one method that we can use to adjust the line to better fit the data.\n",
    "\n",
    "![Absolute trick](extra_images/absolute_trick.png)\n",
    "\n",
    "Note that it's not restricted to adding, subtraction can also be used if we want to lower the line. Note that if `p` is negative then the line rotates in the other direction. `p` can also be close or far to the y-axis, so if it's close then we add a small `p` otherwise we move it by a lot more. Why is `p` so important? it tells us how far it is from y-axis but in terms of **horizontal distance**.\n",
    "\n",
    "### Square trick\n",
    "\n",
    "Solves a problem of before, that `p` is only good for telling us the horizontal distance but what about the vertical? Enter `q`. \n",
    "\n",
    "![Square trick](extra_images/square-trick.png)\n",
    "\n",
    "What is more important, there is only one rule regardless of whether the line is above or below the point compared to the absolute trick where we either had to do addition or subtraction.\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "Gradient descent is a more reliable method for fitting the line based on the error especially during neural networks because we are taking into account the error and we are taking the negative gradient of the derivative (the direction which points towards the largest step towards reducing the error). This is what gradient descent does, it helps us minimize the error by descending towards the minimum value.\n",
    "\n",
    "![Gradient descent](extra_images/gradient-descent.png)\n",
    "\n",
    "\n",
    "How do we actually measure the error?\n",
    "\n",
    "### Mean Absolute Error\n",
    "\n",
    "It's one of the most commonly know methods $Error = \\frac{1}{m}\\sum_{i=1}^{m}|y - \\hat{y}|$\n",
    "\n",
    "The reason why the absolute value is taken from the difference between real value and prediction is that we can obtain the correct value regardless of the values are positive or negative. It's avoiding numbers cancelling each other out.\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "Two important aspects to note:\n",
    "- if we are squaring then we avoid having non-negative numbers\n",
    "- the 1/2 is for convenience as later we are taking the derivative of this error\n",
    "\n",
    "The equation: $Error = \\frac{1}{2m}\\sum_{i=1}^{m}(y - \\hat{y})^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Error Functions\n",
    "\n",
    "The tricks and error functions represent the same thing when using gradient descent to minimize the error.\n",
    "\n",
    "As an example, for the mean squared error function we can develop the derivative of the error function in the following manner.\n",
    "\n",
    "We first define the squared error function: $Error = \\frac{1}{2m}(y - \\hat{y})^2$\n",
    "\n",
    "Also, we define the prediction as $\\hat{y} = w_{1}x + w_{2}$\n",
    "\n",
    "So to calculate the derivative of the error with respect to $w_{1}$ , we simply use the chain rule: $\\frac{\\partial}{\\partial w_{1}} Error = \\frac{\\partial Error}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_{1}}$ \n",
    "\n",
    "Since we already know what the factors are, the error and the prediction our final equation will look like: $\\frac{\\partial}{\\partial w_{1}} Error = (y - \\hat{y})x$\n",
    "\n",
    "- The first factor of the right hand side is the derivative of the Error with respect to the prediction $\\hat{y}$\n",
    "- The second factor is the derivative of the prediction with respect to $w_{1}$, which is simply x.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean vs Total Squared (or Absolute) Error\n",
    "\n",
    "When we have to decide between the two we should remember that:\n",
    "- the total squared error is the sum of errors at each point\n",
    "- the mean squared error is the average of these errors\n",
    "\n",
    "Essentially this mean that the MSE is just a multiple of the total squared error since $M = mT$\n",
    "\n",
    "Also, since derivatives are linear functions it means that the only thing we need to care about is the **learning rate** with which the gradient minimizes the error. It matters less which we select but more what learning rate we use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch vs Stochastic Gradient Descent\n",
    "\n",
    "When we perform gradient descent there are two common ways to go about it. \n",
    "- **Batch Gradient Descent**: We can add these values, update our weights, and then apply the squared (or absolute) trick on the next point. \n",
    "- **Stochastic Gradient Descent**: Or we can calculate these values for all the points, add them, and then update the weights with the sum of these values.\n",
    "\n",
    "![Batch vs Stochastic Gradient Descent](extra_images/batch_stochastic_gd.png)\n",
    "\n",
    "**Keep in mind** if your data is large then both methods are computationally slow which suggests that the data should be partitioned into smaller batches with roughly the same points, leading to the idea of **mini-batch gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60.31564716]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add import statements\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the dataframe to this variable.\n",
    "# TODO: Load the data\n",
    "bmi_life_data = pd.read_csv('data/bmi_and_life_expectancy.csv') \n",
    "#print(bmi_life_data.head(5))\n",
    "#print(bmi_life_data[['BMI']])\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "#TODO: Fit the model and Assign it to bmi_life_model\n",
    "model = LinearRegression()\n",
    "bmi_life_model = model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']]) \n",
    "\n",
    "# Make a prediction using the model\n",
    "# TODO: Predict life expectancy for a BMI value of 21.07931\n",
    "\n",
    "laos_life_exp = bmi_life_model.predict([[21.07931]])\n",
    "print(laos_life_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher dimensions\n",
    "\n",
    "![Higher dimensions](extra_images/higher_dimensions.png)\n",
    "\n",
    "Little changes if we have n-dimensional dataset, we're effectively just expanding the equation with more weights and datapoints.\n",
    "\n",
    "\n",
    "### Multiple linear regression\n",
    "\n",
    "When you have one predictor variable, the equation of the line is $y = mx + b$ \n",
    "\n",
    "Adding a predictor variable to go to two predictor variables means that the predicting equation is: $y = m_{1}x_{1}+ m_{2}x_{2}+ b$\n",
    "\n",
    "You can use more than two predictor variables, with the equation being representted as: $y = m_{1}x_{1}+ m_{2}x_{2}+ m_{3}x_{3} +...+m_{n}x_{n}+b$\n",
    "\n",
    "#### Exercise\n",
    "In this quiz, you'll be using the Boston house-prices dataset. The dataset consists of 13 features of 506 houses and the median home value in $1000's. You'll fit a model on the 13 features to predict the value of the houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.68284712]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "# TODO: Fit the model and assign it to the model variable\n",
    "model = LinearRegression().fit(x, y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "# TODO: Predict housing price for the sample_house\n",
    "prediction = model.predict(sample_house)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about Closed form solution\n",
    "\n",
    "Gradient descent isn't the only way to minimize the mean squared error, this can be achieved in a closed mathematical form. While this can work very well in 2-D data whenever we go beyond that it becomes computationally expensive since finding a solution for the inverse of a matrix $X^TX^ is hard if n is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warnings\n",
    "\n",
    "- Linear Regression Works Best When the Data is Linear (model from the training data. If the relationship in the training data is not really linear, you'll need to either make adjustments or use another kind of model)\n",
    "- Linear Regression is Sensitive to Outliers (If your dataset has some outlying extreme values that don't fit a general pattern, they can have a surprisingly large effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "\n",
    "Can help capture points when the line is not straight with the help of weights that are polynomial.\n",
    "\n",
    "![Polynomial](extra_images/polynomial-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Cheatsheet for regularization\n",
    "\n",
    "![Regularization L1 vs L2](extra_images/cheatsheet_regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks example with [regression](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
