{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Batch Transform outline\n",
    "\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "    - Save data locally\n",
    "    - Upload to S3\n",
    "4. Train a chosen model.\n",
    "    - Set up the training job: https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html\n",
    "    - Execute training job\n",
    "    - Build model\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "    - Set up batch transform job\n",
    "    - Execute batch transform job\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker model components\n",
    "\n",
    "In SageMaker, a model is a collection of information that describes how to perform inference. For the most part, this comprises two very important pieces.\n",
    "\n",
    "The first is the **container** that holds the model inference functionality. For different types of models this code may be different but for simpler models and models provided by Amazon this is typically the same container that was used to train the model.\n",
    "\n",
    "The second is the **model artifacts**. These are the pieces of data that were created during the training process. For example, if we were fitting a linear model then the coefficients that were fit would be saved as model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a model is fit using SageMaker, the process is as follows.\n",
    "\n",
    "First, a compute instance (basically a server somewhere) is started up with the properties that we specified.\n",
    "\n",
    "Next, when the compute instance is ready, the code, in the form of a container, that is used to fit the model is loaded and executed. When this code is executed, it is provided access to the training (and possibly validation) data stored on S3.\n",
    "\n",
    "Once the compute instance has finished fitting the model, the resulting model artifacts are stored on S3 and the compute instance is shut down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis app deployment\n",
    "\n",
    "<img src=\"part-6_images/deployment_prod.png\" alt=\"Deployment schema\" style=\"width: 500px;\"/>\n",
    "\n",
    "The way data flows through the app is as follows. \n",
    "- The user enters a review on our website.\n",
    "- Next, our website sends that data off to an endpoint, created using API Gateway.\n",
    "- Our endpoint acts as an interface to our Lambda function so our user data gets sent to the Lambda function.\n",
    "- Our Lambda function processes the user data and sends it off to the deployed model's endpoint.\n",
    "- The deployed model perform inference on the processed data and returns the inference results to the Lambda function.\n",
    "- The Lambda function returns the results to the original caller using the endpoint constructed using API Gateway.\n",
    "- Lastly, the website receives the inference results and displays those results to the user.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
